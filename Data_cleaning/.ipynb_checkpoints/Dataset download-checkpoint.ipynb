{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410324d2-32ca-4e0f-aea1-15449b280f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6a9ec29-2d47-4eca-a705-3b304b9c8af8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af78b23b44842778fcafd78dfd9f427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Roaming\\Python\\Python313\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ASUS\\.cache\\huggingface\\hub\\datasets--ai4bharat--wiki-translate. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c822b4e83174dfbbdaa9855d8e61f77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 10000 English‚ÄìHindi sentence pairs\n",
      "üíæ Saved file: wiki_translate_en_hi_subset.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Language pair and sample size\n",
    "lang_pair = \"en-hi\"\n",
    "sample_size = 10000  # you can reduce this (e.g., 10000) if low on RAM/storage\n",
    "\n",
    "# Load dataset in streaming mode to avoid full download\n",
    "dataset = load_dataset(\"ai4bharat/wiki-translate\", split=\"train\", streaming=True)\n",
    "\n",
    "# Extract only English ‚Üî Hindi pairs\n",
    "data = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if \"eng_Latn\" in row and \"hin_Deva\" in row:\n",
    "        data.append({\n",
    "            \"src\": row[\"eng_Latn\"],\n",
    "            \"tgt\": row[\"hin_Deva\"]\n",
    "        })\n",
    "        if i + 1 >= sample_size:\n",
    "            break\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"‚úÖ Loaded\", len(df), \"English‚ÄìHindi sentence pairs\")\n",
    "\n",
    "# Save as CSV for preprocessing/training\n",
    "df.to_csv(\"wiki_translate_en_hi_subset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"üíæ Saved file: wiki_translate_en_hi_subset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52886f5e-8d8a-4f09-a02b-9b24558e9956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully!\n",
      "Total rows: 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nBigger (Beyonc√© song)\\n\\n\"Bigger\" (stylized ...</td>\n",
       "      <td>\\n‡§¨‡§°‡§º‡§æ (‡§¨‡•á‡§Ø‡•ã‡§Ç‡§∏‡•á ‡§ó‡•Ä‡§§)\\n\\n\"‡§¨‡§°‡§º‡§æ\" (‡§¨‡§°‡§º‡•á ‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§Æ‡•á‡§Ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nÏù¥Îã§\\n\\nKorean.\\nEtymology 2.\\nVerb.\\nConjugat...</td>\n",
       "      <td>‡§ï‡•ã‡§∞‡§ø‡§Ø‡§æ‡§à‡•§ ‡§µ‡•ç‡§Ø‡•Å‡§§‡•ç‡§™‡§§‡•ç‡§§‡§ø 2. ‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡•§ ‡§∏‡§Ç‡§Ø‡•Å‡§ó‡•ç‡§Æ‡§®‡•§ ‡§®‡•ã‡§ü‡§É...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nHarriet White Fisher\\n\\nHarriet White Fisher...</td>\n",
       "      <td>\\n‡§π‡•à‡§∞‡§ø‡§Ø‡§ü ‡§∏‡§´‡•á‡§¶ ‡§Æ‡§õ‡•Å‡§Ü‡§∞‡§æ\\n\\n‡§π‡•à‡§∞‡§ø‡§Ø‡§ü ‡§µ‡•ç‡§π‡§æ‡§á‡§ü ‡§´‡§ø‡§∂‡§∞ ‡§è‡§Ç‡§°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nIris (mythology)\\n\\nAncient Greek personific...</td>\n",
       "      <td>\\n‡§Ü‡§á‡§∞‡§ø‡§∏ (‡§™‡•å‡§∞‡§æ‡§£‡§ø‡§ï ‡§ï‡§•‡§æ)\\n\\n‡§™‡•ç‡§∞‡§æ‡§ö‡•Ä‡§® ‡§Ø‡•Ç‡§®‡§æ‡§®‡•Ä ‡§ß‡§∞‡•ç‡§Æ ‡§î...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nIoannis Kontoyiannis\\n\\nGreek mathematician ...</td>\n",
       "      <td>\\n‡§á‡§ì‡§®‡§ø‡§∏ ‡§ï‡•ã‡§Ç‡§ü‡•ã‡§Ø‡§®‡§ø‡§∏\\n\\n‡§á‡§ì‡§®‡§ø‡§∏ ‡§ï‡•ã‡§Ç‡§ü‡•ã‡§Ø‡§®‡§ø‡§∏ (‡§ú‡§®‡•ç‡§Æ ‡§ú‡§®‡§µ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0  \\nBigger (Beyonc√© song)\\n\\n\"Bigger\" (stylized ...   \n",
       "1  \\nÏù¥Îã§\\n\\nKorean.\\nEtymology 2.\\nVerb.\\nConjugat...   \n",
       "2  \\nHarriet White Fisher\\n\\nHarriet White Fisher...   \n",
       "3  \\nIris (mythology)\\n\\nAncient Greek personific...   \n",
       "4  \\nIoannis Kontoyiannis\\n\\nGreek mathematician ...   \n",
       "\n",
       "                                                 tgt  \n",
       "0  \\n‡§¨‡§°‡§º‡§æ (‡§¨‡•á‡§Ø‡•ã‡§Ç‡§∏‡•á ‡§ó‡•Ä‡§§)\\n\\n\"‡§¨‡§°‡§º‡§æ\" (‡§¨‡§°‡§º‡•á ‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§Æ‡•á‡§Ç...  \n",
       "1  ‡§ï‡•ã‡§∞‡§ø‡§Ø‡§æ‡§à‡•§ ‡§µ‡•ç‡§Ø‡•Å‡§§‡•ç‡§™‡§§‡•ç‡§§‡§ø 2. ‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡•§ ‡§∏‡§Ç‡§Ø‡•Å‡§ó‡•ç‡§Æ‡§®‡•§ ‡§®‡•ã‡§ü‡§É...  \n",
       "2  \\n‡§π‡•à‡§∞‡§ø‡§Ø‡§ü ‡§∏‡§´‡•á‡§¶ ‡§Æ‡§õ‡•Å‡§Ü‡§∞‡§æ\\n\\n‡§π‡•à‡§∞‡§ø‡§Ø‡§ü ‡§µ‡•ç‡§π‡§æ‡§á‡§ü ‡§´‡§ø‡§∂‡§∞ ‡§è‡§Ç‡§°...  \n",
       "3  \\n‡§Ü‡§á‡§∞‡§ø‡§∏ (‡§™‡•å‡§∞‡§æ‡§£‡§ø‡§ï ‡§ï‡§•‡§æ)\\n\\n‡§™‡•ç‡§∞‡§æ‡§ö‡•Ä‡§® ‡§Ø‡•Ç‡§®‡§æ‡§®‡•Ä ‡§ß‡§∞‡•ç‡§Æ ‡§î...  \n",
       "4  \\n‡§á‡§ì‡§®‡§ø‡§∏ ‡§ï‡•ã‡§Ç‡§ü‡•ã‡§Ø‡§®‡§ø‡§∏\\n\\n‡§á‡§ì‡§®‡§ø‡§∏ ‡§ï‡•ã‡§Ç‡§ü‡•ã‡§Ø‡§®‡§ø‡§∏ (‡§ú‡§®‡•ç‡§Æ ‡§ú‡§®‡§µ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your saved subset\n",
    "df = pd.read_csv(\"wiki_translate_en_hi_subset.csv\")\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101b2b86-90fb-4b38-8b1a-1661b8a42850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Checking for null values:\n",
      "src      0\n",
      "tgt    268\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check how many null or missing values in each column\n",
    "print(\"üîé Checking for null values:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3293a2e3-468f-4e95-8eef-fdc5a9b81e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96557d6d0b58460283e7888339330e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 20000 Hindi-Malyalam sentence pairs\n",
      "üíæ Saved file: wiki_translate_hi_mal_subset.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Language pair and sample size\n",
    "lang_pair = \"hi-mal\"\n",
    "sample_size = 20000  # you can reduce this (e.g., 10000) if low on RAM/storage\n",
    "\n",
    "# Load dataset in streaming mode to avoid full download\n",
    "dataset = load_dataset(\"ai4bharat/wiki-translate\", split=\"train\", streaming=True)\n",
    "\n",
    "# Extract only English ‚Üî Hindi pairs\n",
    "data = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if \"hin_Deva\" in row and \"mal_Mlym\" in row:\n",
    "        data.append({\n",
    "            \"Hindi\": row[\"hin_Deva\"],\n",
    "            \"Malyalam\": row[\"mal_Mlym\"]\n",
    "        })\n",
    "        if i + 1 >= sample_size:\n",
    "            break\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"‚úÖ Loaded\", len(df), \"Hindi-Malyalam sentence pairs\")\n",
    "\n",
    "# Save as CSV for preprocessing/training\n",
    "df.to_csv(\"wiki_translate_hi_mal_subset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"üíæ Saved file: wiki_translate_hi_mal_subset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1b5aaf-b1d4-4123-8a43-cc5fa6864e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully!\n",
      "Total rows: 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hindi</th>\n",
       "      <th>Malyalam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n‡§¨‡§°‡§º‡§æ (‡§¨‡•á‡§Ø‡•ã‡§Ç‡§∏‡•á ‡§ó‡•Ä‡§§)\\n\\n\"‡§¨‡§°‡§º‡§æ\" (‡§¨‡§°‡§º‡•á ‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§Æ‡•á‡§Ç...</td>\n",
       "      <td>\\n‡¥µ‡¥≤‡¥ø‡¥Ø (‡¥¨‡¥ø‡¥Ø‡µã‡µ∫‡¥∏‡µç ‡¥ó‡¥æ‡¥®‡¥Ç)\\n\\n\"2019-‡¥≤‡µÜ ‡¥Ü‡µΩ‡¥¨‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥®‡¥ø‡¥®...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡§ï‡•ã‡§∞‡§ø‡§Ø‡§æ‡§à‡•§ ‡§µ‡•ç‡§Ø‡•Å‡§§‡•ç‡§™‡§§‡•ç‡§§‡§ø 2. ‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡•§ ‡§∏‡§Ç‡§Ø‡•Å‡§ó‡•ç‡§Æ‡§®‡•§ ‡§®‡•ã‡§ü‡§É...</td>\n",
       "      <td>‡¥ï‡µä‡¥±‡¥ø‡¥Ø‡µª. ‡¥™‡¥¶‡¥µ‡µç‡¥Ø‡µÅ‡¥§‡µç‡¥™‡¥§‡µç‡¥§‡¥ø 2. ‡¥ï‡µç‡¥∞‡¥ø‡¥Ø. ‡¥∏‡¥Ç‡¥Ø‡µã‡¥ú‡¥®‡¥Ç. ‡¥ï‡µÅ‡¥±‡¥ø‡¥™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n‡§π‡•à‡§∞‡§ø‡§Ø‡§ü ‡§∏‡§´‡•á‡§¶ ‡§Æ‡§õ‡•Å‡§Ü‡§∞‡§æ\\n\\n‡§π‡•à‡§∞‡§ø‡§Ø‡§ü ‡§µ‡•ç‡§π‡§æ‡§á‡§ü ‡§´‡§ø‡§∂‡§∞ ‡§è‡§Ç‡§°...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n‡§Ü‡§á‡§∞‡§ø‡§∏ (‡§™‡•å‡§∞‡§æ‡§£‡§ø‡§ï ‡§ï‡§•‡§æ)\\n\\n‡§™‡•ç‡§∞‡§æ‡§ö‡•Ä‡§® ‡§Ø‡•Ç‡§®‡§æ‡§®‡•Ä ‡§ß‡§∞‡•ç‡§Æ ‡§î...</td>\n",
       "      <td>\\n‡¥ê‡¥±‡¥ø‡¥∏‡µç (‡¥™‡µÅ‡¥∞‡¥æ‡¥£‡¥Ç)\\n\\n‡¥™‡µÅ‡¥∞‡¥æ‡¥§‡¥® ‡¥ó‡µç‡¥∞‡µÄ‡¥ï‡µç‡¥ï‡µç ‡¥Æ‡¥§‡¥§‡µç‡¥§‡¥ø‡¥≤‡µÅ‡¥Ç ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n‡§á‡§ì‡§®‡§ø‡§∏ ‡§ï‡•ã‡§Ç‡§ü‡•ã‡§Ø‡§®‡§ø‡§∏\\n\\n‡§á‡§ì‡§®‡§ø‡§∏ ‡§ï‡•ã‡§Ç‡§ü‡•ã‡§Ø‡§®‡§ø‡§∏ (‡§ú‡§®‡•ç‡§Æ ‡§ú‡§®‡§µ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Hindi  \\\n",
       "0  \\n‡§¨‡§°‡§º‡§æ (‡§¨‡•á‡§Ø‡•ã‡§Ç‡§∏‡•á ‡§ó‡•Ä‡§§)\\n\\n\"‡§¨‡§°‡§º‡§æ\" (‡§¨‡§°‡§º‡•á ‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§Æ‡•á‡§Ç...   \n",
       "1  ‡§ï‡•ã‡§∞‡§ø‡§Ø‡§æ‡§à‡•§ ‡§µ‡•ç‡§Ø‡•Å‡§§‡•ç‡§™‡§§‡•ç‡§§‡§ø 2. ‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ‡•§ ‡§∏‡§Ç‡§Ø‡•Å‡§ó‡•ç‡§Æ‡§®‡•§ ‡§®‡•ã‡§ü‡§É...   \n",
       "2  \\n‡§π‡•à‡§∞‡§ø‡§Ø‡§ü ‡§∏‡§´‡•á‡§¶ ‡§Æ‡§õ‡•Å‡§Ü‡§∞‡§æ\\n\\n‡§π‡•à‡§∞‡§ø‡§Ø‡§ü ‡§µ‡•ç‡§π‡§æ‡§á‡§ü ‡§´‡§ø‡§∂‡§∞ ‡§è‡§Ç‡§°...   \n",
       "3  \\n‡§Ü‡§á‡§∞‡§ø‡§∏ (‡§™‡•å‡§∞‡§æ‡§£‡§ø‡§ï ‡§ï‡§•‡§æ)\\n\\n‡§™‡•ç‡§∞‡§æ‡§ö‡•Ä‡§® ‡§Ø‡•Ç‡§®‡§æ‡§®‡•Ä ‡§ß‡§∞‡•ç‡§Æ ‡§î...   \n",
       "4  \\n‡§á‡§ì‡§®‡§ø‡§∏ ‡§ï‡•ã‡§Ç‡§ü‡•ã‡§Ø‡§®‡§ø‡§∏\\n\\n‡§á‡§ì‡§®‡§ø‡§∏ ‡§ï‡•ã‡§Ç‡§ü‡•ã‡§Ø‡§®‡§ø‡§∏ (‡§ú‡§®‡•ç‡§Æ ‡§ú‡§®‡§µ...   \n",
       "\n",
       "                                            Malyalam  \n",
       "0  \\n‡¥µ‡¥≤‡¥ø‡¥Ø (‡¥¨‡¥ø‡¥Ø‡µã‡µ∫‡¥∏‡µç ‡¥ó‡¥æ‡¥®‡¥Ç)\\n\\n\"2019-‡¥≤‡µÜ ‡¥Ü‡µΩ‡¥¨‡¥§‡µç‡¥§‡¥ø‡µΩ ‡¥®‡¥ø‡¥®...  \n",
       "1  ‡¥ï‡µä‡¥±‡¥ø‡¥Ø‡µª. ‡¥™‡¥¶‡¥µ‡µç‡¥Ø‡µÅ‡¥§‡µç‡¥™‡¥§‡µç‡¥§‡¥ø 2. ‡¥ï‡µç‡¥∞‡¥ø‡¥Ø. ‡¥∏‡¥Ç‡¥Ø‡µã‡¥ú‡¥®‡¥Ç. ‡¥ï‡µÅ‡¥±‡¥ø‡¥™...  \n",
       "2                                                NaN  \n",
       "3  \\n‡¥ê‡¥±‡¥ø‡¥∏‡µç (‡¥™‡µÅ‡¥∞‡¥æ‡¥£‡¥Ç)\\n\\n‡¥™‡µÅ‡¥∞‡¥æ‡¥§‡¥® ‡¥ó‡µç‡¥∞‡µÄ‡¥ï‡µç‡¥ï‡µç ‡¥Æ‡¥§‡¥§‡µç‡¥§‡¥ø‡¥≤‡µÅ‡¥Ç ...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your saved subset\n",
    "df = pd.read_csv(\"wiki_translate_hi_mal_subset.csv\")\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "992d70af-5008-4e22-8ad9-99139ed284de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Checking for null values:\n",
      "Hindi        536\n",
      "Malyalam    5298\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"üîé Checking for null values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b4b2051-9b87-4e54-8e9b-a798862696a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560d8dabd63f476e83df9521e90e0213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 20000 Bengali-Kannada sentence pairs\n",
      "üíæ Saved file: wiki_translate_ben_kan_subset.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Language pair and sample size\n",
    "lang_pair = \"ben-kan\"\n",
    "sample_size = 20000  # you can reduce this (e.g., 10000) if low on RAM/storage\n",
    "\n",
    "# Load dataset in streaming mode to avoid full download\n",
    "dataset = load_dataset(\"ai4bharat/wiki-translate\", split=\"train\", streaming=True)\n",
    "\n",
    "# Extract only English ‚Üî Hindi pairs\n",
    "data = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if \"ben_Beng\" in row and \"kan_Knda\" in row:\n",
    "        data.append({\n",
    "            \"Bengali\": row[\"ben_Beng\"],\n",
    "            \"Kannada\": row[\"kan_Knda\"]\n",
    "        })\n",
    "        if i + 1 >= sample_size:\n",
    "            break\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"‚úÖ Loaded\", len(df), \"Bengali-Kannada sentence pairs\")\n",
    "\n",
    "# Save as CSV for preprocessing/training\n",
    "df.to_csv(\"wiki_translate_ben_kan_subset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"üíæ Saved file: wiki_translate_ben_kan_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d75e953-8754-4b86-ba6d-f6bf19330914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully!\n",
      "Total rows: 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bengali</th>\n",
       "      <th>Kannada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n‡¶¨‡¶°‡¶º (‡¶¨‡ßá‡¶Ø‡¶º‡ßã‡¶®‡ßç‡¶∏‡ßá ‡¶ó‡¶æ‡¶®)\\n\\n\"‡¶¨‡¶°‡¶º\" (‡¶¨‡¶°‡¶º ‡¶π‡¶æ‡¶§‡ßá‡¶∞ ‡¶∂‡ßà‡¶≤‡ßÄ...</td>\n",
       "      <td>\\n‡≤¶‡≥ä‡≤°‡≥ç‡≤°‡≤¶‡≥Å (‡≤¨‡≤ø‡≤Ø‡≤æ‡≤®‡≥ç‡≤∏‡≥ç ‡≤π‡≤æ‡≤°‡≥Å)\\n\\n\"‡≤¶‡≥ä‡≤°‡≥ç‡≤°‡≤¶‡≥Å\" (‡≤¶‡≥ä‡≤°‡≥ç‡≤° ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡¶ï‡ßã‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶®‡•§ ‡¶¨‡ßç‡¶Ø‡ßÅ‡ßé‡¶™‡¶§‡ßç‡¶§‡¶ø 2. ‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡•§ ‡¶∏‡¶Ç‡¶Æ‡¶ø‡¶∂‡ßç‡¶∞‡¶£‡•§ ‡¶¶‡ßç‡¶∞...</td>\n",
       "      <td>‡≤ï‡≥ä‡≤∞‡≤ø‡≤Ø‡≤®‡≥ç. ‡≤µ‡≥ç‡≤Ø‡≥Å‡≤§‡≥ç‡≤™‡≤§‡≥ç‡≤§‡≤ø 2. ‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≤æ‡≤™‡≤¶. ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ó. ‡≤ó‡≤Æ‡≤®‡≤ø‡≤∏...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n‡¶π‡ßç‡¶Ø‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá‡¶ü ‡¶∏‡¶æ‡¶¶‡¶æ ‡¶Æ‡¶æ‡¶õ‡¶∞‡¶æ‡¶ô‡¶æ\\n\\n‡¶π‡ßç‡¶Ø‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá‡¶ü ‡¶π‡ßã‡¶Ø‡¶º‡¶æ‡¶á...</td>\n",
       "      <td>\\n‡≤π‡≥ç‡≤Ø‡≤æ‡≤∞‡≤ø‡≤Ø‡≥Ü‡≤ü‡≥ç ‡≤¨‡≤ø‡≤≥‡≤ø ‡≤Æ‡≥Ä‡≤®‡≥Å‡≤ó‡≤æ‡≤∞\\n\\n‡≤π‡≥ç‡≤Ø‡≤æ‡≤∞‡≤ø‡≤Ø‡≥Ü‡≤ü‡≥ç ‡≤µ‡≥à‡≤ü‡≥ç ‡≤´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n‡¶Ü‡¶á‡¶∞‡¶ø‡¶∏ (‡¶™‡ßå‡¶∞‡¶æ‡¶£‡¶ø‡¶ï ‡¶ï‡¶æ‡¶π‡¶ø‡¶®‡ßÄ)\\n\\n‡¶™‡ßç‡¶∞‡¶æ‡¶ö‡ßÄ‡¶® ‡¶ó‡ßç‡¶∞‡ßÄ‡¶ï ‡¶ß‡¶∞‡ßç‡¶Æ...</td>\n",
       "      <td>\\n‡≤ê‡≤∞‡≤ø‡≤∏‡≥ç (‡≤™‡≥Å‡≤∞‡≤æ‡≤£)\\n\\n‡≤™‡≥ç‡≤∞‡≤æ‡≤ö‡≥Ä‡≤® ‡≤ó‡≥ç‡≤∞‡≥Ä‡≤ï‡≥ç ‡≤ß‡≤∞‡≥ç‡≤Æ ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n‡¶á‡¶Ø‡¶º‡¶®‡¶ø‡¶∏ ‡¶ï‡¶®‡¶ü‡ßã‡¶Ø‡¶º‡¶ø‡¶Ø‡¶º‡¶æ‡¶®‡ßç‡¶®‡¶ø‡¶∏\\n\\n‡¶á‡¶ì‡¶®‡¶ø‡¶∏ ‡¶ï‡¶®‡¶ü‡ßã‡¶Ø‡¶º‡¶ø‡¶Ø‡¶º‡¶æ‡¶®‡ßç...</td>\n",
       "      <td>\\n‡≤á‡≤Ø‡≥ã‡≤®‡≤ø‡≤∏‡≥ç ‡≤ï‡≥ä‡≤Ç‡≤ü‡≥ä‡≤Ø‡≤®‡≥ç‡≤®‡≤ø‡≤∏‡≥ç\\n\\n‡≤á‡≤Ø‡≥ã‡≤®‡≤ø‡≤∏‡≥ç ‡≤ï‡≥ä‡≤Ç‡≤ü‡≥ä‡≤Ø‡≤®‡≥ç‡≤®‡≤ø‡≤∏‡≥ç...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Bengali  \\\n",
       "0  \\n‡¶¨‡¶°‡¶º (‡¶¨‡ßá‡¶Ø‡¶º‡ßã‡¶®‡ßç‡¶∏‡ßá ‡¶ó‡¶æ‡¶®)\\n\\n\"‡¶¨‡¶°‡¶º\" (‡¶¨‡¶°‡¶º ‡¶π‡¶æ‡¶§‡ßá‡¶∞ ‡¶∂‡ßà‡¶≤‡ßÄ...   \n",
       "1  ‡¶ï‡ßã‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡¶®‡•§ ‡¶¨‡ßç‡¶Ø‡ßÅ‡ßé‡¶™‡¶§‡ßç‡¶§‡¶ø 2. ‡¶ï‡ßç‡¶∞‡¶ø‡¶Ø‡¶º‡¶æ‡•§ ‡¶∏‡¶Ç‡¶Æ‡¶ø‡¶∂‡ßç‡¶∞‡¶£‡•§ ‡¶¶‡ßç‡¶∞...   \n",
       "2  \\n‡¶π‡ßç‡¶Ø‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá‡¶ü ‡¶∏‡¶æ‡¶¶‡¶æ ‡¶Æ‡¶æ‡¶õ‡¶∞‡¶æ‡¶ô‡¶æ\\n\\n‡¶π‡ßç‡¶Ø‡¶æ‡¶∞‡¶ø‡¶Ø‡¶º‡ßá‡¶ü ‡¶π‡ßã‡¶Ø‡¶º‡¶æ‡¶á...   \n",
       "3  \\n‡¶Ü‡¶á‡¶∞‡¶ø‡¶∏ (‡¶™‡ßå‡¶∞‡¶æ‡¶£‡¶ø‡¶ï ‡¶ï‡¶æ‡¶π‡¶ø‡¶®‡ßÄ)\\n\\n‡¶™‡ßç‡¶∞‡¶æ‡¶ö‡ßÄ‡¶® ‡¶ó‡ßç‡¶∞‡ßÄ‡¶ï ‡¶ß‡¶∞‡ßç‡¶Æ...   \n",
       "4  \\n‡¶á‡¶Ø‡¶º‡¶®‡¶ø‡¶∏ ‡¶ï‡¶®‡¶ü‡ßã‡¶Ø‡¶º‡¶ø‡¶Ø‡¶º‡¶æ‡¶®‡ßç‡¶®‡¶ø‡¶∏\\n\\n‡¶á‡¶ì‡¶®‡¶ø‡¶∏ ‡¶ï‡¶®‡¶ü‡ßã‡¶Ø‡¶º‡¶ø‡¶Ø‡¶º‡¶æ‡¶®‡ßç...   \n",
       "\n",
       "                                             Kannada  \n",
       "0  \\n‡≤¶‡≥ä‡≤°‡≥ç‡≤°‡≤¶‡≥Å (‡≤¨‡≤ø‡≤Ø‡≤æ‡≤®‡≥ç‡≤∏‡≥ç ‡≤π‡≤æ‡≤°‡≥Å)\\n\\n\"‡≤¶‡≥ä‡≤°‡≥ç‡≤°‡≤¶‡≥Å\" (‡≤¶‡≥ä‡≤°‡≥ç‡≤° ...  \n",
       "1  ‡≤ï‡≥ä‡≤∞‡≤ø‡≤Ø‡≤®‡≥ç. ‡≤µ‡≥ç‡≤Ø‡≥Å‡≤§‡≥ç‡≤™‡≤§‡≥ç‡≤§‡≤ø 2. ‡≤ï‡≥ç‡≤∞‡≤ø‡≤Ø‡≤æ‡≤™‡≤¶. ‡≤∏‡≤Ç‡≤Ø‡≥ã‡≤ó. ‡≤ó‡≤Æ‡≤®‡≤ø‡≤∏...  \n",
       "2  \\n‡≤π‡≥ç‡≤Ø‡≤æ‡≤∞‡≤ø‡≤Ø‡≥Ü‡≤ü‡≥ç ‡≤¨‡≤ø‡≤≥‡≤ø ‡≤Æ‡≥Ä‡≤®‡≥Å‡≤ó‡≤æ‡≤∞\\n\\n‡≤π‡≥ç‡≤Ø‡≤æ‡≤∞‡≤ø‡≤Ø‡≥Ü‡≤ü‡≥ç ‡≤µ‡≥à‡≤ü‡≥ç ‡≤´...  \n",
       "3  \\n‡≤ê‡≤∞‡≤ø‡≤∏‡≥ç (‡≤™‡≥Å‡≤∞‡≤æ‡≤£)\\n\\n‡≤™‡≥ç‡≤∞‡≤æ‡≤ö‡≥Ä‡≤® ‡≤ó‡≥ç‡≤∞‡≥Ä‡≤ï‡≥ç ‡≤ß‡≤∞‡≥ç‡≤Æ ‡≤Æ‡≤§‡≥ç‡≤§‡≥Å ‡≤™...  \n",
       "4  \\n‡≤á‡≤Ø‡≥ã‡≤®‡≤ø‡≤∏‡≥ç ‡≤ï‡≥ä‡≤Ç‡≤ü‡≥ä‡≤Ø‡≤®‡≥ç‡≤®‡≤ø‡≤∏‡≥ç\\n\\n‡≤á‡≤Ø‡≥ã‡≤®‡≤ø‡≤∏‡≥ç ‡≤ï‡≥ä‡≤Ç‡≤ü‡≥ä‡≤Ø‡≤®‡≥ç‡≤®‡≤ø‡≤∏‡≥ç...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your saved subset\n",
    "df = pd.read_csv(\"wiki_translate_ben_kan_subset.csv\")\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2549f1a8-0583-4bab-a669-e9b1416cc12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Checking for null values:\n",
      "Bengali    1203\n",
      "Kannada    1052\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"üîé Checking for null values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcda69a-0ab8-478b-b05d-38dbbc3a1ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
