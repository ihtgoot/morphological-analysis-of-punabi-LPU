{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "410324d2-32ca-4e0f-aea1-15449b280f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6a9ec29-2d47-4eca-a705-3b304b9c8af8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842f5f81eb9f42bc8349e62884268fc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Loaded 10000 EnglishтАУHindi sentence pairs\n",
      "ЁЯТ╛ Saved file: wiki_translate_en_hi_subset.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Language pair and sample size\n",
    "lang_pair = \"en-hi\"\n",
    "sample_size = 10000  # you can reduce this (e.g., 10000) if low on RAM/storage\n",
    "\n",
    "# Load dataset in streaming mode to avoid full download\n",
    "dataset = load_dataset(\"ai4bharat/wiki-translate\", split=\"train\", streaming=True)\n",
    "\n",
    "# Extract only English тЖФ Hindi pairs\n",
    "data = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if \"eng_Latn\" in row and \"hin_Deva\" in row:\n",
    "        data.append({\n",
    "            \"src\": row[\"eng_Latn\"],\n",
    "            \"tgt\": row[\"hin_Deva\"]\n",
    "        })\n",
    "        if i + 1 >= sample_size:\n",
    "            break\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"тЬЕ Loaded\", len(df), \"EnglishтАУHindi sentence pairs\")\n",
    "\n",
    "# Save as CSV for preprocessing/training\n",
    "df.to_csv(\"wiki_translate_en_hi_subset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"ЁЯТ╛ Saved file: wiki_translate_en_hi_subset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52886f5e-8d8a-4f09-a02b-9b24558e9956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Dataset loaded successfully!\n",
      "Total rows: 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nBigger (Beyonc├й song)\\n\\n\"Bigger\" (stylized ...</td>\n",
       "      <td>\\nрдмрдбрд╝рд╛ (рдмреЗрдпреЛрдВрд╕реЗ рдЧреАрдд)\\n\\n\"рдмрдбрд╝рд╛\" (рдмрдбрд╝реЗ рдЕрдХреНрд╖рд░ рдореЗрдВ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nьЭ┤ыЛд\\n\\nKorean.\\nEtymology 2.\\nVerb.\\nConjugat...</td>\n",
       "      <td>рдХреЛрд░рд┐рдпрд╛рдИред рд╡реНрдпреБрддреНрдкрддреНрддрд┐ 2. рдХреНрд░рд┐рдпрд╛ред рд╕рдВрдпреБрдЧреНрдордиред рдиреЛрдЯрдГ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nHarriet White Fisher\\n\\nHarriet White Fisher...</td>\n",
       "      <td>\\nрд╣реИрд░рд┐рдпрдЯ рд╕рдлреЗрдж рдордЫреБрдЖрд░рд╛\\n\\nрд╣реИрд░рд┐рдпрдЯ рд╡реНрд╣рд╛рдЗрдЯ рдлрд┐рд╢рд░ рдПрдВрдб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nIris (mythology)\\n\\nAncient Greek personific...</td>\n",
       "      <td>\\nрдЖрдЗрд░рд┐рд╕ (рдкреМрд░рд╛рдгрд┐рдХ рдХрдерд╛)\\n\\nрдкреНрд░рд╛рдЪреАрди рдпреВрдирд╛рдиреА рдзрд░реНрдо рдФ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nIoannis Kontoyiannis\\n\\nGreek mathematician ...</td>\n",
       "      <td>\\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕\\n\\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕ (рдЬрдиреНрдо рдЬрдирд╡...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 src  \\\n",
       "0  \\nBigger (Beyonc├й song)\\n\\n\"Bigger\" (stylized ...   \n",
       "1  \\nьЭ┤ыЛд\\n\\nKorean.\\nEtymology 2.\\nVerb.\\nConjugat...   \n",
       "2  \\nHarriet White Fisher\\n\\nHarriet White Fisher...   \n",
       "3  \\nIris (mythology)\\n\\nAncient Greek personific...   \n",
       "4  \\nIoannis Kontoyiannis\\n\\nGreek mathematician ...   \n",
       "\n",
       "                                                 tgt  \n",
       "0  \\nрдмрдбрд╝рд╛ (рдмреЗрдпреЛрдВрд╕реЗ рдЧреАрдд)\\n\\n\"рдмрдбрд╝рд╛\" (рдмрдбрд╝реЗ рдЕрдХреНрд╖рд░ рдореЗрдВ...  \n",
       "1  рдХреЛрд░рд┐рдпрд╛рдИред рд╡реНрдпреБрддреНрдкрддреНрддрд┐ 2. рдХреНрд░рд┐рдпрд╛ред рд╕рдВрдпреБрдЧреНрдордиред рдиреЛрдЯрдГ...  \n",
       "2  \\nрд╣реИрд░рд┐рдпрдЯ рд╕рдлреЗрдж рдордЫреБрдЖрд░рд╛\\n\\nрд╣реИрд░рд┐рдпрдЯ рд╡реНрд╣рд╛рдЗрдЯ рдлрд┐рд╢рд░ рдПрдВрдб...  \n",
       "3  \\nрдЖрдЗрд░рд┐рд╕ (рдкреМрд░рд╛рдгрд┐рдХ рдХрдерд╛)\\n\\nрдкреНрд░рд╛рдЪреАрди рдпреВрдирд╛рдиреА рдзрд░реНрдо рдФ...  \n",
       "4  \\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕\\n\\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕ (рдЬрдиреНрдо рдЬрдирд╡...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your saved subset\n",
    "df = pd.read_csv(\"wiki_translate_en_hi_subset.csv\")\n",
    "\n",
    "print(\"тЬЕ Dataset loaded successfully!\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101b2b86-90fb-4b38-8b1a-1661b8a42850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯФО Checking for null values:\n",
      "src      0\n",
      "tgt    268\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check how many null or missing values in each column\n",
    "print(\"ЁЯФО Checking for null values:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3293a2e3-468f-4e95-8eef-fdc5a9b81e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96557d6d0b58460283e7888339330e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Loaded 20000 Hindi-Malyalam sentence pairs\n",
      "ЁЯТ╛ Saved file: wiki_translate_hi_mal_subset.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Language pair and sample size\n",
    "lang_pair = \"hi-mal\"\n",
    "sample_size = 20000  # you can reduce this (e.g., 10000) if low on RAM/storage\n",
    "\n",
    "# Load dataset in streaming mode to avoid full download\n",
    "dataset = load_dataset(\"ai4bharat/wiki-translate\", split=\"train\", streaming=True)\n",
    "\n",
    "# Extract only English тЖФ Hindi pairs\n",
    "data = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if \"hin_Deva\" in row and \"mal_Mlym\" in row:\n",
    "        data.append({\n",
    "            \"Hindi\": row[\"hin_Deva\"],\n",
    "            \"Malyalam\": row[\"mal_Mlym\"]\n",
    "        })\n",
    "        if i + 1 >= sample_size:\n",
    "            break\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"тЬЕ Loaded\", len(df), \"Hindi-Malyalam sentence pairs\")\n",
    "\n",
    "# Save as CSV for preprocessing/training\n",
    "df.to_csv(\"wiki_translate_hi_mal_subset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"ЁЯТ╛ Saved file: wiki_translate_hi_mal_subset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f1b5aaf-b1d4-4123-8a43-cc5fa6864e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Dataset loaded successfully!\n",
      "Total rows: 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hindi</th>\n",
       "      <th>Malyalam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nрдмрдбрд╝рд╛ (рдмреЗрдпреЛрдВрд╕реЗ рдЧреАрдд)\\n\\n\"рдмрдбрд╝рд╛\" (рдмрдбрд╝реЗ рдЕрдХреНрд╖рд░ рдореЗрдВ...</td>\n",
       "      <td>\\nр┤╡р┤▓р┤┐р┤п (р┤мр┤┐р┤пр╡Лр╡║р┤╕р╡Н р┤Чр┤╛р┤ир┤В)\\n\\n\"2019-р┤▓р╡Ж р┤Жр╡╜р┤мр┤др╡Нр┤др┤┐р╡╜ р┤ир┤┐р┤и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>рдХреЛрд░рд┐рдпрд╛рдИред рд╡реНрдпреБрддреНрдкрддреНрддрд┐ 2. рдХреНрд░рд┐рдпрд╛ред рд╕рдВрдпреБрдЧреНрдордиред рдиреЛрдЯрдГ...</td>\n",
       "      <td>р┤Хр╡Кр┤▒р┤┐р┤пр╡╗. р┤кр┤жр┤╡р╡Нр┤пр╡Бр┤др╡Нр┤кр┤др╡Нр┤др┤┐ 2. р┤Хр╡Нр┤░р┤┐р┤п. р┤╕р┤Вр┤пр╡Лр┤Ьр┤ир┤В. р┤Хр╡Бр┤▒р┤┐р┤к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nрд╣реИрд░рд┐рдпрдЯ рд╕рдлреЗрдж рдордЫреБрдЖрд░рд╛\\n\\nрд╣реИрд░рд┐рдпрдЯ рд╡реНрд╣рд╛рдЗрдЯ рдлрд┐рд╢рд░ рдПрдВрдб...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nрдЖрдЗрд░рд┐рд╕ (рдкреМрд░рд╛рдгрд┐рдХ рдХрдерд╛)\\n\\nрдкреНрд░рд╛рдЪреАрди рдпреВрдирд╛рдиреА рдзрд░реНрдо рдФ...</td>\n",
       "      <td>\\nр┤Рр┤▒р┤┐р┤╕р╡Н (р┤кр╡Бр┤░р┤╛р┤гр┤В)\\n\\nр┤кр╡Бр┤░р┤╛р┤др┤и р┤Чр╡Нр┤░р╡Ар┤Хр╡Нр┤Хр╡Н р┤ор┤др┤др╡Нр┤др┤┐р┤▓р╡Бр┤В ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕\\n\\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕ (рдЬрдиреНрдо рдЬрдирд╡...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Hindi  \\\n",
       "0  \\nрдмрдбрд╝рд╛ (рдмреЗрдпреЛрдВрд╕реЗ рдЧреАрдд)\\n\\n\"рдмрдбрд╝рд╛\" (рдмрдбрд╝реЗ рдЕрдХреНрд╖рд░ рдореЗрдВ...   \n",
       "1  рдХреЛрд░рд┐рдпрд╛рдИред рд╡реНрдпреБрддреНрдкрддреНрддрд┐ 2. рдХреНрд░рд┐рдпрд╛ред рд╕рдВрдпреБрдЧреНрдордиред рдиреЛрдЯрдГ...   \n",
       "2  \\nрд╣реИрд░рд┐рдпрдЯ рд╕рдлреЗрдж рдордЫреБрдЖрд░рд╛\\n\\nрд╣реИрд░рд┐рдпрдЯ рд╡реНрд╣рд╛рдЗрдЯ рдлрд┐рд╢рд░ рдПрдВрдб...   \n",
       "3  \\nрдЖрдЗрд░рд┐рд╕ (рдкреМрд░рд╛рдгрд┐рдХ рдХрдерд╛)\\n\\nрдкреНрд░рд╛рдЪреАрди рдпреВрдирд╛рдиреА рдзрд░реНрдо рдФ...   \n",
       "4  \\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕\\n\\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕ (рдЬрдиреНрдо рдЬрдирд╡...   \n",
       "\n",
       "                                            Malyalam  \n",
       "0  \\nр┤╡р┤▓р┤┐р┤п (р┤мр┤┐р┤пр╡Лр╡║р┤╕р╡Н р┤Чр┤╛р┤ир┤В)\\n\\n\"2019-р┤▓р╡Ж р┤Жр╡╜р┤мр┤др╡Нр┤др┤┐р╡╜ р┤ир┤┐р┤и...  \n",
       "1  р┤Хр╡Кр┤▒р┤┐р┤пр╡╗. р┤кр┤жр┤╡р╡Нр┤пр╡Бр┤др╡Нр┤кр┤др╡Нр┤др┤┐ 2. р┤Хр╡Нр┤░р┤┐р┤п. р┤╕р┤Вр┤пр╡Лр┤Ьр┤ир┤В. р┤Хр╡Бр┤▒р┤┐р┤к...  \n",
       "2                                                NaN  \n",
       "3  \\nр┤Рр┤▒р┤┐р┤╕р╡Н (р┤кр╡Бр┤░р┤╛р┤гр┤В)\\n\\nр┤кр╡Бр┤░р┤╛р┤др┤и р┤Чр╡Нр┤░р╡Ар┤Хр╡Нр┤Хр╡Н р┤ор┤др┤др╡Нр┤др┤┐р┤▓р╡Бр┤В ...  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your saved subset\n",
    "df = pd.read_csv(\"wiki_translate_hi_mal_subset.csv\")\n",
    "\n",
    "print(\"тЬЕ Dataset loaded successfully!\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "992d70af-5008-4e22-8ad9-99139ed284de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯФО Checking for null values:\n",
      "Hindi        536\n",
      "Malyalam    5298\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"ЁЯФО Checking for null values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b4b2051-9b87-4e54-8e9b-a798862696a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560d8dabd63f476e83df9521e90e0213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Loaded 20000 Bengali-Kannada sentence pairs\n",
      "ЁЯТ╛ Saved file: wiki_translate_ben_kan_subset.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Language pair and sample size\n",
    "lang_pair = \"ben-kan\"\n",
    "sample_size = 20000  # you can reduce this (e.g., 10000) if low on RAM/storage\n",
    "\n",
    "# Load dataset in streaming mode to avoid full download\n",
    "dataset = load_dataset(\"ai4bharat/wiki-translate\", split=\"train\", streaming=True)\n",
    "\n",
    "# Extract only English тЖФ Hindi pairs\n",
    "data = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if \"ben_Beng\" in row and \"kan_Knda\" in row:\n",
    "        data.append({\n",
    "            \"Bengali\": row[\"ben_Beng\"],\n",
    "            \"Kannada\": row[\"kan_Knda\"]\n",
    "        })\n",
    "        if i + 1 >= sample_size:\n",
    "            break\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"тЬЕ Loaded\", len(df), \"Bengali-Kannada sentence pairs\")\n",
    "\n",
    "# Save as CSV for preprocessing/training\n",
    "df.to_csv(\"wiki_translate_ben_kan_subset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"ЁЯТ╛ Saved file: wiki_translate_ben_kan_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d75e953-8754-4b86-ba6d-f6bf19330914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Dataset loaded successfully!\n",
      "Total rows: 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bengali</th>\n",
       "      <th>Kannada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nржмржбрж╝ (ржмрзЗржпрж╝рзЛржирзНрж╕рзЗ ржЧрж╛ржи)\\n\\n\"ржмржбрж╝\" (ржмржбрж╝ рж╣рж╛рждрзЗрж░ рж╢рзИрж▓рзА...</td>\n",
       "      <td>\\nр▓жр│Кр▓бр│Нр▓бр▓жр│Б (р▓мр▓┐р▓пр▓╛р▓ир│Нр▓╕р│Н р▓╣р▓╛р▓бр│Б)\\n\\n\"р▓жр│Кр▓бр│Нр▓бр▓жр│Б\" (р▓жр│Кр▓бр│Нр▓б ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ржХрзЛрж░рж┐ржпрж╝рж╛ржиред ржмрзНржпрзБрзОржкрждрзНрждрж┐ 2. ржХрзНрж░рж┐ржпрж╝рж╛ред рж╕ржВржорж┐рж╢рзНрж░ржгред ржжрзНрж░...</td>\n",
       "      <td>р▓Хр│Кр▓░р▓┐р▓пр▓ир│Н. р▓╡р│Нр▓пр│Бр▓др│Нр▓кр▓др│Нр▓др▓┐ 2. р▓Хр│Нр▓░р▓┐р▓пр▓╛р▓кр▓ж. р▓╕р▓Вр▓пр│Лр▓Ч. р▓Чр▓ор▓ир▓┐р▓╕...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nрж╣рзНржпрж╛рж░рж┐ржпрж╝рзЗржЯ рж╕рж╛ржжрж╛ ржорж╛ржЫрж░рж╛ржЩрж╛\\n\\nрж╣рзНржпрж╛рж░рж┐ржпрж╝рзЗржЯ рж╣рзЛржпрж╝рж╛ржЗ...</td>\n",
       "      <td>\\nр▓╣р│Нр▓пр▓╛р▓░р▓┐р▓пр│Жр▓Яр│Н р▓мр▓┐р▓│р▓┐ р▓ор│Ар▓ир│Бр▓Чр▓╛р▓░\\n\\nр▓╣р│Нр▓пр▓╛р▓░р▓┐р▓пр│Жр▓Яр│Н р▓╡р│Ир▓Яр│Н р▓л...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nржЖржЗрж░рж┐рж╕ (ржкрзМрж░рж╛ржгрж┐ржХ ржХрж╛рж╣рж┐ржирзА)\\n\\nржкрзНрж░рж╛ржЪрзАржи ржЧрзНрж░рзАржХ ржзрж░рзНржо...</td>\n",
       "      <td>\\nр▓Рр▓░р▓┐р▓╕р│Н (р▓кр│Бр▓░р▓╛р▓г)\\n\\nр▓кр│Нр▓░р▓╛р▓Ър│Ар▓и р▓Чр│Нр▓░р│Ар▓Хр│Н р▓зр▓░р│Нр▓о р▓ор▓др│Нр▓др│Б р▓к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nржЗржпрж╝ржирж┐рж╕ ржХржиржЯрзЛржпрж╝рж┐ржпрж╝рж╛ржирзНржирж┐рж╕\\n\\nржЗржУржирж┐рж╕ ржХржиржЯрзЛржпрж╝рж┐ржпрж╝рж╛ржирзН...</td>\n",
       "      <td>\\nр▓Зр▓пр│Лр▓ир▓┐р▓╕р│Н р▓Хр│Кр▓Вр▓Яр│Кр▓пр▓ир│Нр▓ир▓┐р▓╕р│Н\\n\\nр▓Зр▓пр│Лр▓ир▓┐р▓╕р│Н р▓Хр│Кр▓Вр▓Яр│Кр▓пр▓ир│Нр▓ир▓┐р▓╕р│Н...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Bengali  \\\n",
       "0  \\nржмржбрж╝ (ржмрзЗржпрж╝рзЛржирзНрж╕рзЗ ржЧрж╛ржи)\\n\\n\"ржмржбрж╝\" (ржмржбрж╝ рж╣рж╛рждрзЗрж░ рж╢рзИрж▓рзА...   \n",
       "1  ржХрзЛрж░рж┐ржпрж╝рж╛ржиред ржмрзНржпрзБрзОржкрждрзНрждрж┐ 2. ржХрзНрж░рж┐ржпрж╝рж╛ред рж╕ржВржорж┐рж╢рзНрж░ржгред ржжрзНрж░...   \n",
       "2  \\nрж╣рзНржпрж╛рж░рж┐ржпрж╝рзЗржЯ рж╕рж╛ржжрж╛ ржорж╛ржЫрж░рж╛ржЩрж╛\\n\\nрж╣рзНржпрж╛рж░рж┐ржпрж╝рзЗржЯ рж╣рзЛржпрж╝рж╛ржЗ...   \n",
       "3  \\nржЖржЗрж░рж┐рж╕ (ржкрзМрж░рж╛ржгрж┐ржХ ржХрж╛рж╣рж┐ржирзА)\\n\\nржкрзНрж░рж╛ржЪрзАржи ржЧрзНрж░рзАржХ ржзрж░рзНржо...   \n",
       "4  \\nржЗржпрж╝ржирж┐рж╕ ржХржиржЯрзЛржпрж╝рж┐ржпрж╝рж╛ржирзНржирж┐рж╕\\n\\nржЗржУржирж┐рж╕ ржХржиржЯрзЛржпрж╝рж┐ржпрж╝рж╛ржирзН...   \n",
       "\n",
       "                                             Kannada  \n",
       "0  \\nр▓жр│Кр▓бр│Нр▓бр▓жр│Б (р▓мр▓┐р▓пр▓╛р▓ир│Нр▓╕р│Н р▓╣р▓╛р▓бр│Б)\\n\\n\"р▓жр│Кр▓бр│Нр▓бр▓жр│Б\" (р▓жр│Кр▓бр│Нр▓б ...  \n",
       "1  р▓Хр│Кр▓░р▓┐р▓пр▓ир│Н. р▓╡р│Нр▓пр│Бр▓др│Нр▓кр▓др│Нр▓др▓┐ 2. р▓Хр│Нр▓░р▓┐р▓пр▓╛р▓кр▓ж. р▓╕р▓Вр▓пр│Лр▓Ч. р▓Чр▓ор▓ир▓┐р▓╕...  \n",
       "2  \\nр▓╣р│Нр▓пр▓╛р▓░р▓┐р▓пр│Жр▓Яр│Н р▓мр▓┐р▓│р▓┐ р▓ор│Ар▓ир│Бр▓Чр▓╛р▓░\\n\\nр▓╣р│Нр▓пр▓╛р▓░р▓┐р▓пр│Жр▓Яр│Н р▓╡р│Ир▓Яр│Н р▓л...  \n",
       "3  \\nр▓Рр▓░р▓┐р▓╕р│Н (р▓кр│Бр▓░р▓╛р▓г)\\n\\nр▓кр│Нр▓░р▓╛р▓Ър│Ар▓и р▓Чр│Нр▓░р│Ар▓Хр│Н р▓зр▓░р│Нр▓о р▓ор▓др│Нр▓др│Б р▓к...  \n",
       "4  \\nр▓Зр▓пр│Лр▓ир▓┐р▓╕р│Н р▓Хр│Кр▓Вр▓Яр│Кр▓пр▓ир│Нр▓ир▓┐р▓╕р│Н\\n\\nр▓Зр▓пр│Лр▓ир▓┐р▓╕р│Н р▓Хр│Кр▓Вр▓Яр│Кр▓пр▓ир│Нр▓ир▓┐р▓╕р│Н...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your saved subset\n",
    "df = pd.read_csv(\"wiki_translate_ben_kan_subset.csv\")\n",
    "\n",
    "print(\"тЬЕ Dataset loaded successfully!\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2549f1a8-0583-4bab-a669-e9b1416cc12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯФО Checking for null values:\n",
      "Bengali    1203\n",
      "Kannada    1052\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"ЁЯФО Checking for null values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dcda69a-0ab8-478b-b05d-38dbbc3a1ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55012d5c02224a0f8f378da44a49d0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Loaded 20000 Telgu_Tamil sentence pairs\n",
      "ЁЯТ╛ Saved file: wiki_translate_tel_tamil_subset.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Language pair and sample size\n",
    "lang_pair = \"tel-tam\"\n",
    "sample_size = 20000  # you can reduce this (e.g., 10000) if low on RAM/storage\n",
    "\n",
    "# Load dataset in streaming mode to avoid full download\n",
    "dataset = load_dataset(\"ai4bharat/wiki-translate\", split=\"train\", streaming=True)\n",
    "\n",
    "# Extract only English тЖФ Hindi pairs\n",
    "data = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if \"tel_Telu\" in row and \"tam_Taml\" in row:\n",
    "        data.append({\n",
    "            \"Telgu\": row[\"tel_Telu\"],\n",
    "            \"Tamil\": row[\"tam_Taml\"]\n",
    "        })\n",
    "        if i + 1 >= sample_size:\n",
    "            break\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"тЬЕ Loaded\", len(df), \"Telgu_Tamil sentence pairs\")\n",
    "\n",
    "# Save as CSV for preprocessing/training\n",
    "df.to_csv(\"wiki_translate_tel_tamil_subset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"ЁЯТ╛ Saved file: wiki_translate_tel_tamil_subset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c020a9e-8294-4783-b9d7-43aec37bd508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Dataset loaded successfully!\n",
      "Total rows: 20000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Telgu</th>\n",
       "      <th>Tamil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nр░кр▒Жр░жр▒Нр░жр░жр░┐ (р░мр░┐р░пр░╛р░ир▒Нр░╕р▒Н р░кр░╛р░Я)\\n\\n\"р░кр▒Жр░жр▒Нр░жр░жр░┐\" (р░кр▒Жр░жр▒Нр░ж р░Е...</td>\n",
       "      <td>\\nрокрпЖро░ро┐роп (рокро┐ропрпЛройро╕рпН рокро╛роЯро▓рпН)\\n\\n\"рокрпЖро░ро┐ропродрпБ\" (рокрпЖро░ро┐роп роОро┤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>р░Хр▒Кр░░р░┐р░пр░ир▒Н. р░╡р▒Нр░пр▒Бр░др▒Нр░кр░др▒Нр░др░┐ р░╢р░╛р░╕р▒Нр░др▒Нр░░р░В 2. р░Хр▒Нр░░р░┐р░п. р░╕р░Вр░пр▒Лр░Чр░В...</td>\n",
       "      <td>роХрпКро░ро┐ропройрпН. роЪрпКро▒рпНрокро┐ро▒рокрпНрокро┐ропро▓рпН 2. ро╡ро┐ройрпИроЪрпНроЪрпКро▓рпН. роЗрогрпИродрпНродро▓...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nр░╣р░╛р░░р░┐р░пр░Яр▒Н р░╡р▒Ир░Яр▒Н р░лр░┐р░╖р░░р▒Н\\n\\nр░╣р░╛р░░р░┐р░пр▒Жр░Яр▒Н р░╡р▒Ир░Яр▒Н р░лр░┐р░╖р░░р▒Н р░Жр░В...</td>\n",
       "      <td>\\nро╣ро╛ро░ро┐ропроЯрпН ро╡рпЖро│рпНро│рпИ роорпАройро╡ро░рпН\\n\\nро╣ро╛ро░ро┐ропроЯрпН ро╡рпЖро│рпНро│рпИ роорпАройро╡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nр░Рр░░р░┐р░╕р▒Н (р░кр▒Бр░░р░╛р░гр░В)\\n\\nр░кр▒Бр░░р░╛р░др░и р░Чр▒Нр░░р▒Ар░Хр▒Б р░ор░др░В р░ор░░р░┐р░пр▒Б р░кр▒Б...</td>\n",
       "      <td>\\nроРро░ро┐ро╕рпН (рокрпБро░ро╛рогроорпН)\\n\\nрокрогрпНроЯрпИроп роХро┐ро░рпЗроХрпНроХ роородроорпН рооро▒рпНро▒рпБ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nр░Зр░пр▒Лр░ир░┐р░╕р▒Н р░Хр▒Кр░Вр░Яр▒Лр░пр░ир▒Нр░ир░┐р░╕р▒Н\\n\\nр░Зр░пр▒Лр░ир░┐р░╕р▒Н р░Хр▒Кр░Вр░Яр▒Лр░пр░╛р░ир▒Нр░ир░┐р░╕...</td>\n",
       "      <td>\\nроЕропрпЛройро┐ро╕рпН роХрпКройрпНроЯрпЛропро┐ропройрпНройро┐ро╕рпН\\n\\nроЕропрпЛройро┐ро╕рпН роХрпКройрпНроЯрпЛропро╛рой...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Telgu  \\\n",
       "0  \\nр░кр▒Жр░жр▒Нр░жр░жр░┐ (р░мр░┐р░пр░╛р░ир▒Нр░╕р▒Н р░кр░╛р░Я)\\n\\n\"р░кр▒Жр░жр▒Нр░жр░жр░┐\" (р░кр▒Жр░жр▒Нр░ж р░Е...   \n",
       "1  р░Хр▒Кр░░р░┐р░пр░ир▒Н. р░╡р▒Нр░пр▒Бр░др▒Нр░кр░др▒Нр░др░┐ р░╢р░╛р░╕р▒Нр░др▒Нр░░р░В 2. р░Хр▒Нр░░р░┐р░п. р░╕р░Вр░пр▒Лр░Чр░В...   \n",
       "2  \\nр░╣р░╛р░░р░┐р░пр░Яр▒Н р░╡р▒Ир░Яр▒Н р░лр░┐р░╖р░░р▒Н\\n\\nр░╣р░╛р░░р░┐р░пр▒Жр░Яр▒Н р░╡р▒Ир░Яр▒Н р░лр░┐р░╖р░░р▒Н р░Жр░В...   \n",
       "3  \\nр░Рр░░р░┐р░╕р▒Н (р░кр▒Бр░░р░╛р░гр░В)\\n\\nр░кр▒Бр░░р░╛р░др░и р░Чр▒Нр░░р▒Ар░Хр▒Б р░ор░др░В р░ор░░р░┐р░пр▒Б р░кр▒Б...   \n",
       "4  \\nр░Зр░пр▒Лр░ир░┐р░╕р▒Н р░Хр▒Кр░Вр░Яр▒Лр░пр░ир▒Нр░ир░┐р░╕р▒Н\\n\\nр░Зр░пр▒Лр░ир░┐р░╕р▒Н р░Хр▒Кр░Вр░Яр▒Лр░пр░╛р░ир▒Нр░ир░┐р░╕...   \n",
       "\n",
       "                                               Tamil  \n",
       "0  \\nрокрпЖро░ро┐роп (рокро┐ропрпЛройро╕рпН рокро╛роЯро▓рпН)\\n\\n\"рокрпЖро░ро┐ропродрпБ\" (рокрпЖро░ро┐роп роОро┤...  \n",
       "1  роХрпКро░ро┐ропройрпН. роЪрпКро▒рпНрокро┐ро▒рокрпНрокро┐ропро▓рпН 2. ро╡ро┐ройрпИроЪрпНроЪрпКро▓рпН. роЗрогрпИродрпНродро▓...  \n",
       "2  \\nро╣ро╛ро░ро┐ропроЯрпН ро╡рпЖро│рпНро│рпИ роорпАройро╡ро░рпН\\n\\nро╣ро╛ро░ро┐ропроЯрпН ро╡рпЖро│рпНро│рпИ роорпАройро╡...  \n",
       "3  \\nроРро░ро┐ро╕рпН (рокрпБро░ро╛рогроорпН)\\n\\nрокрогрпНроЯрпИроп роХро┐ро░рпЗроХрпНроХ роородроорпН рооро▒рпНро▒рпБ...  \n",
       "4  \\nроЕропрпЛройро┐ро╕рпН роХрпКройрпНроЯрпЛропро┐ропройрпНройро┐ро╕рпН\\n\\nроЕропрпЛройро┐ро╕рпН роХрпКройрпНроЯрпЛропро╛рой...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your saved subset\n",
    "df = pd.read_csv(\"wiki_translate_tel_tamil_subset.csv\")\n",
    "\n",
    "print(\"тЬЕ Dataset loaded successfully!\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0324d11-5483-4ee8-8209-adc4ca4bf86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯФО Checking for null values:\n",
      "Telgu    1566\n",
      "Tamil    1235\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"ЁЯФО Checking for null values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c4e11bb-369b-4ce4-946e-d55ca3bd7634",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12550474ffa3484b88d224587fc0438a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Loaded 15000 rows with English, Hindi, Malayalam, and Tamil sentences\n",
      "ЁЯТ╛ Saved file: wiki_translate_en_hi_ml_ta.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Sample size (adjust based on your RAM)\n",
    "sample_size = 15000  \n",
    "\n",
    "# Load the dataset in streaming mode\n",
    "dataset = load_dataset(\"ai4bharat/wiki-translate\", split=\"train\", streaming=True)\n",
    "\n",
    "# Initialize data list\n",
    "data = []\n",
    "\n",
    "# Extract English, Hindi, Malayalam, and Tamil sentences\n",
    "for i, row in enumerate(dataset):\n",
    "    if all(lang in row for lang in [\"eng_Latn\", \"hin_Deva\", \"mal_Mlym\", \"tam_Taml\"]):\n",
    "        data.append({\n",
    "            \"english\": row[\"eng_Latn\"],\n",
    "            \"hindi\": row[\"hin_Deva\"],\n",
    "            \"malayalam\": row[\"mal_Mlym\"],\n",
    "            \"tamil\": row[\"tam_Taml\"]\n",
    "        })\n",
    "        if len(data) >= sample_size:\n",
    "            break\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display number of rows\n",
    "print(f\"тЬЕ Loaded {len(df)} rows with English, Hindi, Malayalam, and Tamil sentences\")\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(\"wiki_translate_en_hi_ml_ta.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"ЁЯТ╛ Saved file: wiki_translate_en_hi_ml_ta.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d46a3809-6048-4f3b-89ee-189eee331429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "тЬЕ Dataset loaded successfully!\n",
      "Total rows: 15000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>english</th>\n",
       "      <th>hindi</th>\n",
       "      <th>malayalam</th>\n",
       "      <th>tamil</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nBigger (Beyonc├й song)\\n\\n\"Bigger\" (stylized ...</td>\n",
       "      <td>\\nрдмрдбрд╝рд╛ (рдмреЗрдпреЛрдВрд╕реЗ рдЧреАрдд)\\n\\n\"рдмрдбрд╝рд╛\" (рдмрдбрд╝реЗ рдЕрдХреНрд╖рд░ рдореЗрдВ...</td>\n",
       "      <td>\\nр┤╡р┤▓р┤┐р┤п (р┤мр┤┐р┤пр╡Лр╡║р┤╕р╡Н р┤Чр┤╛р┤ир┤В)\\n\\n\"2019-р┤▓р╡Ж р┤Жр╡╜р┤мр┤др╡Нр┤др┤┐р╡╜ р┤ир┤┐р┤и...</td>\n",
       "      <td>\\nрокрпЖро░ро┐роп (рокро┐ропрпЛройро╕рпН рокро╛роЯро▓рпН)\\n\\n\"рокрпЖро░ро┐ропродрпБ\" (рокрпЖро░ро┐роп роОро┤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nьЭ┤ыЛд\\n\\nKorean.\\nEtymology 2.\\nVerb.\\nConjugat...</td>\n",
       "      <td>рдХреЛрд░рд┐рдпрд╛рдИред рд╡реНрдпреБрддреНрдкрддреНрддрд┐ 2. рдХреНрд░рд┐рдпрд╛ред рд╕рдВрдпреБрдЧреНрдордиред рдиреЛрдЯрдГ...</td>\n",
       "      <td>р┤Хр╡Кр┤▒р┤┐р┤пр╡╗. р┤кр┤жр┤╡р╡Нр┤пр╡Бр┤др╡Нр┤кр┤др╡Нр┤др┤┐ 2. р┤Хр╡Нр┤░р┤┐р┤п. р┤╕р┤Вр┤пр╡Лр┤Ьр┤ир┤В. р┤Хр╡Бр┤▒р┤┐р┤к...</td>\n",
       "      <td>роХрпКро░ро┐ропройрпН. роЪрпКро▒рпНрокро┐ро▒рокрпНрокро┐ропро▓рпН 2. ро╡ро┐ройрпИроЪрпНроЪрпКро▓рпН. роЗрогрпИродрпНродро▓...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\nHarriet White Fisher\\n\\nHarriet White Fisher...</td>\n",
       "      <td>\\nрд╣реИрд░рд┐рдпрдЯ рд╕рдлреЗрдж рдордЫреБрдЖрд░рд╛\\n\\nрд╣реИрд░рд┐рдпрдЯ рд╡реНрд╣рд╛рдЗрдЯ рдлрд┐рд╢рд░ рдПрдВрдб...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nро╣ро╛ро░ро┐ропроЯрпН ро╡рпЖро│рпНро│рпИ роорпАройро╡ро░рпН\\n\\nро╣ро╛ро░ро┐ропроЯрпН ро╡рпЖро│рпНро│рпИ роорпАройро╡...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nIris (mythology)\\n\\nAncient Greek personific...</td>\n",
       "      <td>\\nрдЖрдЗрд░рд┐рд╕ (рдкреМрд░рд╛рдгрд┐рдХ рдХрдерд╛)\\n\\nрдкреНрд░рд╛рдЪреАрди рдпреВрдирд╛рдиреА рдзрд░реНрдо рдФ...</td>\n",
       "      <td>\\nр┤Рр┤▒р┤┐р┤╕р╡Н (р┤кр╡Бр┤░р┤╛р┤гр┤В)\\n\\nр┤кр╡Бр┤░р┤╛р┤др┤и р┤Чр╡Нр┤░р╡Ар┤Хр╡Нр┤Хр╡Н р┤ор┤др┤др╡Нр┤др┤┐р┤▓р╡Бр┤В ...</td>\n",
       "      <td>\\nроРро░ро┐ро╕рпН (рокрпБро░ро╛рогроорпН)\\n\\nрокрогрпНроЯрпИроп роХро┐ро░рпЗроХрпНроХ роородроорпН рооро▒рпНро▒рпБ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\nIoannis Kontoyiannis\\n\\nGreek mathematician ...</td>\n",
       "      <td>\\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕\\n\\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕ (рдЬрдиреНрдо рдЬрдирд╡...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\nроЕропрпЛройро┐ро╕рпН роХрпКройрпНроЯрпЛропро┐ропройрпНройро┐ро╕рпН\\n\\nроЕропрпЛройро┐ро╕рпН роХрпКройрпНроЯрпЛропро╛рой...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             english  \\\n",
       "0  \\nBigger (Beyonc├й song)\\n\\n\"Bigger\" (stylized ...   \n",
       "1  \\nьЭ┤ыЛд\\n\\nKorean.\\nEtymology 2.\\nVerb.\\nConjugat...   \n",
       "2  \\nHarriet White Fisher\\n\\nHarriet White Fisher...   \n",
       "3  \\nIris (mythology)\\n\\nAncient Greek personific...   \n",
       "4  \\nIoannis Kontoyiannis\\n\\nGreek mathematician ...   \n",
       "\n",
       "                                               hindi  \\\n",
       "0  \\nрдмрдбрд╝рд╛ (рдмреЗрдпреЛрдВрд╕реЗ рдЧреАрдд)\\n\\n\"рдмрдбрд╝рд╛\" (рдмрдбрд╝реЗ рдЕрдХреНрд╖рд░ рдореЗрдВ...   \n",
       "1  рдХреЛрд░рд┐рдпрд╛рдИред рд╡реНрдпреБрддреНрдкрддреНрддрд┐ 2. рдХреНрд░рд┐рдпрд╛ред рд╕рдВрдпреБрдЧреНрдордиред рдиреЛрдЯрдГ...   \n",
       "2  \\nрд╣реИрд░рд┐рдпрдЯ рд╕рдлреЗрдж рдордЫреБрдЖрд░рд╛\\n\\nрд╣реИрд░рд┐рдпрдЯ рд╡реНрд╣рд╛рдЗрдЯ рдлрд┐рд╢рд░ рдПрдВрдб...   \n",
       "3  \\nрдЖрдЗрд░рд┐рд╕ (рдкреМрд░рд╛рдгрд┐рдХ рдХрдерд╛)\\n\\nрдкреНрд░рд╛рдЪреАрди рдпреВрдирд╛рдиреА рдзрд░реНрдо рдФ...   \n",
       "4  \\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕\\n\\nрдЗрдУрдирд┐рд╕ рдХреЛрдВрдЯреЛрдпрдирд┐рд╕ (рдЬрдиреНрдо рдЬрдирд╡...   \n",
       "\n",
       "                                           malayalam  \\\n",
       "0  \\nр┤╡р┤▓р┤┐р┤п (р┤мр┤┐р┤пр╡Лр╡║р┤╕р╡Н р┤Чр┤╛р┤ир┤В)\\n\\n\"2019-р┤▓р╡Ж р┤Жр╡╜р┤мр┤др╡Нр┤др┤┐р╡╜ р┤ир┤┐р┤и...   \n",
       "1  р┤Хр╡Кр┤▒р┤┐р┤пр╡╗. р┤кр┤жр┤╡р╡Нр┤пр╡Бр┤др╡Нр┤кр┤др╡Нр┤др┤┐ 2. р┤Хр╡Нр┤░р┤┐р┤п. р┤╕р┤Вр┤пр╡Лр┤Ьр┤ир┤В. р┤Хр╡Бр┤▒р┤┐р┤к...   \n",
       "2                                                NaN   \n",
       "3  \\nр┤Рр┤▒р┤┐р┤╕р╡Н (р┤кр╡Бр┤░р┤╛р┤гр┤В)\\n\\nр┤кр╡Бр┤░р┤╛р┤др┤и р┤Чр╡Нр┤░р╡Ар┤Хр╡Нр┤Хр╡Н р┤ор┤др┤др╡Нр┤др┤┐р┤▓р╡Бр┤В ...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                               tamil  \n",
       "0  \\nрокрпЖро░ро┐роп (рокро┐ропрпЛройро╕рпН рокро╛роЯро▓рпН)\\n\\n\"рокрпЖро░ро┐ропродрпБ\" (рокрпЖро░ро┐роп роОро┤...  \n",
       "1  роХрпКро░ро┐ропройрпН. роЪрпКро▒рпНрокро┐ро▒рокрпНрокро┐ропро▓рпН 2. ро╡ро┐ройрпИроЪрпНроЪрпКро▓рпН. роЗрогрпИродрпНродро▓...  \n",
       "2  \\nро╣ро╛ро░ро┐ропроЯрпН ро╡рпЖро│рпНро│рпИ роорпАройро╡ро░рпН\\n\\nро╣ро╛ро░ро┐ропроЯрпН ро╡рпЖро│рпНро│рпИ роорпАройро╡...  \n",
       "3  \\nроРро░ро┐ро╕рпН (рокрпБро░ро╛рогроорпН)\\n\\nрокрогрпНроЯрпИроп роХро┐ро░рпЗроХрпНроХ роородроорпН рооро▒рпНро▒рпБ...  \n",
       "4  \\nроЕропрпЛройро┐ро╕рпН роХрпКройрпНроЯрпЛропро┐ропройрпНройро┐ро╕рпН\\n\\nроЕропрпЛройро┐ро╕рпН роХрпКройрпНроЯрпЛропро╛рой...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your saved subset\n",
    "df = pd.read_csv(\"wiki_translate_en_hi_ml_ta.csv\")\n",
    "\n",
    "print(\"тЬЕ Dataset loaded successfully!\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ab11841-ea23-4a52-9d8b-0d2c4ebf1fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ЁЯФО Checking for null values:\n",
      "english         0\n",
      "hindi         410\n",
      "malayalam    3981\n",
      "tamil         914\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"ЁЯФО Checking for null values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdb0c3-ff96-4643-8f2b-3c20393e97fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
