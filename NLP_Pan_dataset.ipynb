{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2255e1c9-389f-405d-ba90-56552e0fe0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78bf152f76a4114b7f5494e9cc023a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.' thrown while requesting GET https://huggingface.co/datasets/ai4bharat/wiki-translate/resolve/68badfb8db907f2c0ebda8480a08ccfc0313def4/train/wiki_train_0003_of_0255.parquet\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 0255f416-da86-41d7-b468-1c855c855fc7)')' thrown while requesting GET https://huggingface.co/datasets/ai4bharat/wiki-translate/resolve/68badfb8db907f2c0ebda8480a08ccfc0313def4/train/wiki_train_0003_of_0255.parquet\n",
      "Retrying in 2s [Retry 2/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/ai4bharat/wiki-translate/resolve/68badfb8db907f2c0ebda8480a08ccfc0313def4/train/wiki_train_0003_of_0255.parquet (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x00000205145D8410>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: b94478d0-0d79-4b49-a98c-f5eb54ea988e)')' thrown while requesting GET https://huggingface.co/datasets/ai4bharat/wiki-translate/resolve/68badfb8db907f2c0ebda8480a08ccfc0313def4/train/wiki_train_0003_of_0255.parquet\n",
      "Retrying in 4s [Retry 3/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/ai4bharat/wiki-translate/resolve/68badfb8db907f2c0ebda8480a08ccfc0313def4/train/wiki_train_0003_of_0255.parquet (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000002051447BD90>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 1a865f8c-9584-4925-a1e4-bf1040ffddd6)')' thrown while requesting GET https://huggingface.co/datasets/ai4bharat/wiki-translate/resolve/68badfb8db907f2c0ebda8480a08ccfc0313def4/train/wiki_train_0003_of_0255.parquet\n",
      "Retrying in 8s [Retry 4/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/ai4bharat/wiki-translate/resolve/68badfb8db907f2c0ebda8480a08ccfc0313def4/train/wiki_train_0003_of_0255.parquet (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000020567BFA5D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 66d7a7b4-a375-4681-a804-1710cfde882a)')' thrown while requesting GET https://huggingface.co/datasets/ai4bharat/wiki-translate/resolve/68badfb8db907f2c0ebda8480a08ccfc0313def4/train/wiki_train_0003_of_0255.parquet\n",
      "Retrying in 8s [Retry 5/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /datasets/ai4bharat/wiki-translate/resolve/68badfb8db907f2c0ebda8480a08ccfc0313def4/train/wiki_train_0003_of_0255.parquet (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000020567BFAAD0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: f35bf101-84a2-469c-af91-81a38a32c90b)')' thrown while requesting GET https://huggingface.co/datasets/ai4bharat/wiki-translate/resolve/68badfb8db907f2c0ebda8480a08ccfc0313def4/train/wiki_train_0003_of_0255.parquet\n",
      "Got disconnected from remote data host. Retrying in 5sec [1/20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 100000 Punjabi Datset : \n",
      "üíæ Saved file: wiki_translate_punjabi_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "sample_size = 100000  # you can reduce this (e.g., 10000) if low on RAM/storage\n",
    "\n",
    "# Load dataset in streaming mode to avoid full download\n",
    "dataset = load_dataset(\"ai4bharat/wiki-translate\", split=\"train\", streaming=True)\n",
    "\n",
    "# Extract only English ‚Üî Hindi pairs\n",
    "data = []\n",
    "for i, row in enumerate(dataset):\n",
    "    if \"pan_Guru\" in row:\n",
    "        data.append({\n",
    "            \"Punjabi\": row[\"pan_Guru\"]\n",
    "        })\n",
    "        if i + 1 >= sample_size:\n",
    "            break\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "print(\"‚úÖ Loaded\", len(df), \"Punjabi Datset : \")\n",
    "\n",
    "# Save as CSV for preprocessing/training\n",
    "df.to_csv(\"wiki_translate_punjabi_dataset.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"üíæ Saved file: wiki_translate_punjabi_dataset.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08bec9bc-1f83-4c8a-886f-0a26f9e8f9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully!\n",
      "Total rows: 100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Punjabi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n‡®µ‡©±‡®°‡®æ (‡®¨‡©á‡®ì‡®®‡®∏‡©á ‡®ó‡©Ä‡®§)\\n\\n\"‡®µ‡©±‡®°‡®æ\" (‡®µ‡©±‡®°‡©á ‡®Ö‡©±‡®ñ‡®∞ ‡®µ‡®ø‡©±‡®ö ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>‡®ï‡©ã‡®∞‡©Ä‡®Ü‡®à. ‡®∏‡®º‡®¨‡®¶-‡®µ‡®ø‡®ó‡®ø‡®Ü‡®® 2. ‡®ï‡©ç‡®∞‡®ø‡®Ü‡•§ ‡®∏‡©∞‡®Ø‡©ã‡®ú‡®®. ‡®®‡©ã‡®ü‡®É ‡®π‡®æ‡®≤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n‡®π‡©à‡®∞‡©Ä‡®è‡®ü ‡®ö‡®ø‡©±‡®ü‡®æ ‡®Æ‡®õ‡©á‡®∞‡®æ\\n\\n‡®π‡©à‡®∞‡©Ä‡®è‡®ü ‡®µ‡©ç‡®π‡®æ‡®à‡®ü ‡®´‡®ø‡®∏‡®º‡®∞ ‡®ê‡®Ç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n‡®Ü‡®à‡®∞‡®ø‡®∏ (‡®Æ‡®ø‡®•‡®ø‡®π‡®æ‡®∏)\\n\\n‡®™‡©ç‡®∞‡®æ‡®ö‡©Ä‡®® ‡®Ø‡©Ç‡®®‡®æ‡®®‡©Ä ‡®ß‡®∞‡®Æ ‡®Ö‡®§‡©á ‡®Æ‡®ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n‡®Ü‡®á‡®ì‡®®‡®ø‡®∏ ‡®ï‡©ã‡®Ç‡®ü‡©ã‡®á‡®Ü‡®®‡®ø‡®∏\\n\\n‡®á‡®ì‡®®‡®ø‡®∏ ‡®ï‡©ã‡®Ç‡®ü‡©ã‡®á‡®Ü‡®®‡®ø‡®∏ (‡®ú‡®®‡®Æ ‡®ú...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Punjabi\n",
       "0  \\n‡®µ‡©±‡®°‡®æ (‡®¨‡©á‡®ì‡®®‡®∏‡©á ‡®ó‡©Ä‡®§)\\n\\n\"‡®µ‡©±‡®°‡®æ\" (‡®µ‡©±‡®°‡©á ‡®Ö‡©±‡®ñ‡®∞ ‡®µ‡®ø‡©±‡®ö ...\n",
       "1  ‡®ï‡©ã‡®∞‡©Ä‡®Ü‡®à. ‡®∏‡®º‡®¨‡®¶-‡®µ‡®ø‡®ó‡®ø‡®Ü‡®® 2. ‡®ï‡©ç‡®∞‡®ø‡®Ü‡•§ ‡®∏‡©∞‡®Ø‡©ã‡®ú‡®®. ‡®®‡©ã‡®ü‡®É ‡®π‡®æ‡®≤...\n",
       "2  \\n‡®π‡©à‡®∞‡©Ä‡®è‡®ü ‡®ö‡®ø‡©±‡®ü‡®æ ‡®Æ‡®õ‡©á‡®∞‡®æ\\n\\n‡®π‡©à‡®∞‡©Ä‡®è‡®ü ‡®µ‡©ç‡®π‡®æ‡®à‡®ü ‡®´‡®ø‡®∏‡®º‡®∞ ‡®ê‡®Ç...\n",
       "3  \\n‡®Ü‡®à‡®∞‡®ø‡®∏ (‡®Æ‡®ø‡®•‡®ø‡®π‡®æ‡®∏)\\n\\n‡®™‡©ç‡®∞‡®æ‡®ö‡©Ä‡®® ‡®Ø‡©Ç‡®®‡®æ‡®®‡©Ä ‡®ß‡®∞‡®Æ ‡®Ö‡®§‡©á ‡®Æ‡®ø...\n",
       "4  \\n‡®Ü‡®á‡®ì‡®®‡®ø‡®∏ ‡®ï‡©ã‡®Ç‡®ü‡©ã‡®á‡®Ü‡®®‡®ø‡®∏\\n\\n‡®á‡®ì‡®®‡®ø‡®∏ ‡®ï‡©ã‡®Ç‡®ü‡©ã‡®á‡®Ü‡®®‡®ø‡®∏ (‡®ú‡®®‡®Æ ‡®ú..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your saved subset\n",
    "df = pd.read_csv(\"wiki_translate_punjabi_dataset.csv\")\n",
    "\n",
    "print(\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7e1a82f-1a5e-4386-bc84-f22b7385611f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Checking for null values:\n",
      "Punjabi    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"üîé Checking for null values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "715a101a-80e9-460e-9d85-f70848f7ea15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "580a74e2-4321-4b1d-8c93-01b25e5718fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PUNJABI DATASET CLEANING PIPELINE\n",
      "======================================================================\n",
      "Loading CSV from: wiki_translate_punjabi_dataset.csv\n",
      "Original dataset shape: (100000, 1)\n",
      "Columns: ['Punjabi']\n",
      "Using column: 'Punjabi'\n",
      "\n",
      "Cleaning data...\n",
      "After cleaning: 99973 rows remaining\n",
      "\n",
      "--- Sample Cleaned Text ---\n",
      "1. ‡®µ‡©±‡®°‡®æ (‡®¨‡©á‡®ì‡®®‡®∏‡©á ‡®ó‡©Ä‡®§) \"‡®µ‡©±‡®°‡®æ\" (‡®µ‡©±‡®°‡©á ‡®Ö‡©±‡®ñ‡®∞ ‡®µ‡®ø‡©±‡®ö ‡®∏‡®º‡©à‡®≤‡©Ä‡®¨‡©±‡®ß) ‡®Ö‡®Æ‡®∞‡©Ä‡®ï‡©Ä ‡®ó‡®æ‡®á‡®ï ‡®¨‡®ø‡®ì‡®®‡®∏‡©á ‡®¶‡©Å‡®Ü‡®∞‡®æ 2019 ‡®¶‡©Ä ‡®ê‡®≤‡®¨‡®Æ \"\" ‡®§‡©ã‡®Ç ‡®á‡©±‡®ï ‡®ó‡©Ä‡®§ ‡®π‡©à ‡®Ö‡®§‡©á 2020 ‡®¶‡©Ä ‡®´‡®ø‡®≤‡®Æ \"\" ‡®¨‡®≤‡©à‡®ï ‡®á‡®ú‡®º ‡®ï‡®ø‡©∞‡®ó \"\" ‡®µ‡®ø‡©±‡®ö ‡®™‡©ç‡®∞‡®¶‡®∞‡®∏‡®º‡®ø‡®§ ‡®ï‡©Ä‡®§‡®æ ‡®ó‡®ø‡®Ü ‡®π‡©à‡•§ ‡®™‡®ø‡®õ‡©ã‡®ï‡®°‡®º. ‡®á‡®π ‡®ó‡©Ä‡®§ ‡®¨‡®ø‡®ì‡®®‡®∏‡©á ‡®¶‡©á ‡®∏‡®ü...\n",
      "2. ‡®ï‡©ã‡®∞‡©Ä‡®Ü‡®à. ‡®∏‡®º‡®¨‡®¶-‡®µ‡®ø‡®ó‡®ø‡®Ü‡®® 2. ‡®ï‡©ç‡®∞‡®ø‡®Ü‡•§ ‡®∏‡©∞‡®Ø‡©ã‡®ú‡®®. ‡®®‡©ã‡®ü‡®É ‡®π‡®æ‡®≤‡®æ‡®Ç‡®ï‡®ø ‡®§‡®ú‡®µ‡©Ä‡®ú‡®º ‡®Ö‡®®‡©Å‡®∏‡®æ‡®∞ ‡®∏‡©∞‡®≠‡®µ ‡®π‡©à, ‡®´‡®æ‡®∞‡®Æ ‡®Ö‡®≠‡®ø‡®Ü‡®∏ ‡®µ‡®ø‡©±‡®ö ‡®ò‡©±‡®ü ‡®π‡©Ä ‡®µ‡®∞‡®§‡©á ‡®ú‡®æ‡®Ç‡®¶‡©á ‡®π‡®®‡•§ (, ‡®Ü‡®¶‡®ø) ‡®¶‡©á ‡®∞‡©Ç‡®™ ‡®µ‡®ø‡©±‡®ö ‡®¶‡®ø‡©±‡®§‡©á ‡®ó‡®è ‡®∏‡®æ‡®∞‡©á ‡®∞‡©Ç‡®™‡®æ‡®Ç ‡®≤‡®à‡•§) ‡®π‡©á‡®†‡®æ‡®Ç, ‡®®‡©ã‡®ü ‡®ï‡®∞‡©ã ‡®ï‡®ø ‡®Ü‡®Æ ‡®∞‡©Ç‡®™ (, ‡®Ü‡®¶‡®ø) ‡®π‡©à‡•§)‡•§ ‡®∏‡®º‡®¨‡®¶...\n",
      "3. ‡®π‡©à‡®∞‡©Ä‡®è‡®ü ‡®ö‡®ø‡©±‡®ü‡®æ ‡®Æ‡®õ‡©á‡®∞‡®æ ‡®π‡©à‡®∞‡©Ä‡®è‡®ü ‡®µ‡©ç‡®π‡®æ‡®à‡®ü ‡®´‡®ø‡®∏‡®º‡®∞ ‡®ê‡®Ç‡®°‡®∞‡®ø‡®ä (1861-1939) ‡®á‡©±‡®ï ‡®Ö‡®Æ‡®∞‡©Ä‡®ï‡©Ä ‡®∏‡©Ä ‡®ú‡©ã ‡®á‡©±‡®ï ‡®≤‡©ã‡®ï‡©ã‡®Æ‡©ã‡®¨‡®æ‡®à‡®≤ ‡®µ‡®ø‡©±‡®ö ‡®¶‡©Å‡®®‡©Ä‡®Ü ‡®¶‡®æ ‡®ö‡©±‡®ï‡®∞ ‡®≤‡®ó‡®æ‡®â‡®£ ‡®µ‡®æ‡®≤‡©Ä ‡®™‡®π‡®ø‡®≤‡©Ä ‡®î‡®∞‡®§ ‡®µ‡®ú‡©ã‡®Ç ‡®ú‡®æ‡®£‡©Ä ‡®ú‡®æ‡®Ç‡®¶‡©Ä ‡®∏‡©Ä‡•§ ‡®ú‡®®‡®Æ ‡®Ö‡®§‡©á ‡®Æ‡©Å‡®¢‡®≤‡®æ ‡®ú‡©Ä‡®µ‡®®‡•§ ‡®π‡©à‡®∞‡©Ä‡®è‡®ü ‡®µ‡©ç‡®π‡®æ‡®à‡®ü ‡®¶‡®æ ‡®ú‡®®‡®Æ 31 ‡®Æ‡®æ‡®∞‡®ö 1...\n",
      "\n",
      "Saving cleaned data to: punjabi_cleaned.txt\n",
      "Saved 99973 texts to punjabi_cleaned.txt\n",
      "\n",
      "======================================================================\n",
      "CLEANING COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Output file:\n",
      "  - Cleaned text: punjabi_cleaned.txt\n",
      "  - Total lines saved: 2421603\n",
      "\n",
      "You can now use this file for tokenization!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "class PunjabiDataCleaner:\n",
    "    \"\"\"\n",
    "    Comprehensive data cleaner for Punjabi Wikipedia text data.\n",
    "    Handles common cleaning tasks for Indic language text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Punjabi Unicode range: \\u0A00-\\u0A7F\n",
    "        self.punjabi_pattern = re.compile(r'[\\u0A00-\\u0A7F]')\n",
    "        \n",
    "    def remove_urls(self, text):\n",
    "        \"\"\"Remove URLs from text\"\"\"\n",
    "        url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        return re.sub(url_pattern, '', text)\n",
    "    \n",
    "    def remove_email(self, text):\n",
    "        \"\"\"Remove email addresses\"\"\"\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        return re.sub(email_pattern, '', text)\n",
    "    \n",
    "    def remove_html_tags(self, text):\n",
    "        \"\"\"Remove HTML tags\"\"\"\n",
    "        html_pattern = re.compile(r'<.*?>')\n",
    "        return html_pattern.sub('', text)\n",
    "    \n",
    "    def remove_special_chars(self, text):\n",
    "        \"\"\"Remove special characters but keep Punjabi characters and basic punctuation\"\"\"\n",
    "        # Keep Punjabi chars, spaces, common punctuation\n",
    "        pattern = r'[^\\u0A00-\\u0A7F\\s‡•§‡••,.!?;:()\\-\\'\\\"0-9a-zA-Z]'\n",
    "        return re.sub(pattern, ' ', text)\n",
    "    \n",
    "    def normalize_whitespace(self, text):\n",
    "        \"\"\"Normalize multiple spaces/tabs/newlines to single space\"\"\"\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def remove_extra_punctuation(self, text):\n",
    "        \"\"\"Remove repeated punctuation marks\"\"\"\n",
    "        text = re.sub(r'([‡•§‡••,.!?;:])\\1+', r'\\1', text)\n",
    "        return text\n",
    "    \n",
    "    def normalize_unicode(self, text):\n",
    "        \"\"\"Normalize Unicode characters (NFC normalization)\"\"\"\n",
    "        return unicodedata.normalize('NFC', text)\n",
    "    \n",
    "    def remove_digits_only_lines(self, text):\n",
    "        \"\"\"Remove lines that contain only digits\"\"\"\n",
    "        if re.match(r'^\\d+$', text.strip()):\n",
    "            return ''\n",
    "        return text\n",
    "    \n",
    "    def has_punjabi_content(self, text):\n",
    "        \"\"\"Check if text contains Punjabi characters\"\"\"\n",
    "        return bool(self.punjabi_pattern.search(text))\n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"\n",
    "        Apply all cleaning steps to a single text string\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        \n",
    "        # Convert to string\n",
    "        text = str(text)\n",
    "        \n",
    "        # Apply cleaning steps in order\n",
    "        text = self.remove_urls(text)\n",
    "        text = self.remove_email(text)\n",
    "        text = self.remove_html_tags(text)\n",
    "        text = self.normalize_unicode(text)\n",
    "        text = self.remove_special_chars(text)\n",
    "        text = self.remove_extra_punctuation(text)\n",
    "        text = self.normalize_whitespace(text)\n",
    "        text = self.remove_digits_only_lines(text)\n",
    "        \n",
    "        # Keep only if it has Punjabi content and is not too short\n",
    "        if self.has_punjabi_content(text) and len(text.strip()) > 10:\n",
    "            return text\n",
    "        \n",
    "        return ''\n",
    "\n",
    "\n",
    "def load_and_clean_csv(csv_path, column_name=None, sample_size=None):\n",
    "    \"\"\"\n",
    "    Load CSV file and clean the data\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to CSV file\n",
    "        column_name: Name of the column containing text (if None, uses first column)\n",
    "        sample_size: Number of rows to sample (if None, uses all rows)\n",
    "    \"\"\"\n",
    "    print(f\"Loading CSV from: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\"Original dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    \n",
    "    # Use specified column or first column\n",
    "    if column_name is None:\n",
    "        column_name = df.columns[0]\n",
    "    \n",
    "    print(f\"Using column: '{column_name}'\")\n",
    "    \n",
    "    # Sample if needed\n",
    "    if sample_size and sample_size < len(df):\n",
    "        df = df.sample(n=sample_size, random_state=42)\n",
    "        print(f\"Sampled {sample_size} rows\")\n",
    "    \n",
    "    # Initialize cleaner\n",
    "    cleaner = PunjabiDataCleaner()\n",
    "    \n",
    "    # Clean the data\n",
    "    print(\"\\nCleaning data...\")\n",
    "    df['cleaned_text'] = df[column_name].apply(cleaner.clean_text)\n",
    "    \n",
    "    # Remove empty strings\n",
    "    df = df[df['cleaned_text'] != '']\n",
    "    \n",
    "    print(f\"After cleaning: {len(df)} rows remaining\")\n",
    "    \n",
    "    return df['cleaned_text'].tolist()\n",
    "\n",
    "\n",
    "def save_to_txt(texts, output_path):\n",
    "    \"\"\"\n",
    "    Save cleaned texts to a .txt file (one sentence per line)\n",
    "    \"\"\"\n",
    "    print(f\"\\nSaving cleaned data to: {output_path}\")\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for text in texts:\n",
    "            # Split into sentences if needed (by Punjabi danda)\n",
    "            sentences = re.split(r'[‡•§‡••]', text)\n",
    "            for sent in sentences:\n",
    "                sent = sent.strip()\n",
    "                if sent and len(sent) > 5:  # Only save non-empty sentences\n",
    "                    f.write(sent + '\\n')\n",
    "    \n",
    "    print(f\"Saved {len(texts)} texts to {output_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Configuration - UPDATE THESE PATHS!\n",
    "    # Example 1: File in same directory\n",
    "    CSV_FILE = \"wiki_translate_punjabi_dataset.csv\"  # Change to your CSV filename\n",
    "    \n",
    "    # Example 2: Full path (Windows)\n",
    "    # CSV_FILE = r\"C:\\Users\\YourName\\Downloads\\wiki_translate_punjabi.csv\"\n",
    "    \n",
    "    # Example 3: Full path (Linux/Mac)\n",
    "    # CSV_FILE = \"/home/username/Downloads/wiki_translate_punjabi.csv\"\n",
    "    \n",
    "    COLUMN_NAME = None  # Set to None to use first column, or specify like \"text\" or \"content\"\n",
    "    OUTPUT_TXT = \"punjabi_cleaned.txt\"  # Output file path\n",
    "    \n",
    "    # Optional: Use a sample for testing (set to None to use all data)\n",
    "    SAMPLE_SIZE = None  # e.g., 10000 for testing, None for full dataset\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"PUNJABI DATASET CLEANING PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Step 1: Load and clean the data\n",
    "    try:\n",
    "        cleaned_texts = load_and_clean_csv(\n",
    "            csv_path=CSV_FILE,\n",
    "            column_name=COLUMN_NAME,\n",
    "            sample_size=SAMPLE_SIZE\n",
    "        )\n",
    "        \n",
    "        # Display sample\n",
    "        print(\"\\n--- Sample Cleaned Text ---\")\n",
    "        for i, text in enumerate(cleaned_texts[:3], 1):\n",
    "            print(f\"{i}. {text[:200]}...\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{CSV_FILE}' not found!\")\n",
    "        print(\"Please make sure the CSV file is in the same directory.\")\n",
    "        exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Step 2: Save to text file\n",
    "    try:\n",
    "        save_to_txt(cleaned_texts, OUTPUT_TXT)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving to text file: {e}\")\n",
    "        exit(1)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CLEANING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nOutput file:\")\n",
    "    print(f\"  - Cleaned text: {OUTPUT_TXT}\")\n",
    "    print(f\"  - Total lines saved: {sum(1 for _ in open(OUTPUT_TXT, encoding='utf-8'))}\")\n",
    "    print(\"\\nYou can now use this file for tokenization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2b648-9352-494a-8ad3-5ea2a2508642",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
